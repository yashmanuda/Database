{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashmanuda/Database/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDdKVOMtDNtH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
        "\n",
        "import tensorflow\n",
        "import numpy as np\n",
        "import string\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "training_size = 12528566  # if you change the size to 1000 it will work if to_use_sparse is False or is_convert is False\n",
        "\n",
        "to_use_sparse = True  # if set True and to_convert is False, throws error that adapter is not found\n",
        "\n",
        "X_train = np.random.randint(low=0, high=169999, size=(training_size, 10), dtype='int32')\n",
        "labels = []\n",
        "for i in range(3811):\n",
        "    labels.append(''.join(random.choices(string.ascii_uppercase + string.digits, k=6)))\n",
        "Y = [random.choice(labels) for i in range(training_size)]\n",
        "\n",
        "if to_use_sparse:\n",
        "    Y_train = LabelBinarizer(sparse_output=True).fit(labels).transform(Y)  # no adapter is found\n",
        "else:\n",
        "    Y_train = LabelBinarizer(sparse_output=True).fit(labels).transform(Y).toarray()  # this thing works but only if training_size is small, say 1000\n",
        "\n",
        "    \n",
        "to_convert = True # converting the labels to integers of their index and using sparse_categorical_crossentropy\n",
        "\n",
        "if to_convert and to_use_sparse: # if true, it works as expected\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=170000, output_dim=100, input_length=10))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dense(3811, 'softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "    model.fit(X_train,\n",
        "              np.asarray(Y_train.tocoo().col),\n",
        "              batch_size=16384,\n",
        "              epochs=5,\n",
        "              verbose=1)\n",
        "else:\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=170000, output_dim=100, input_length=10))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dense(3811, 'softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "    model.fit(X_train,\n",
        "              Y_train,\n",
        "              batch_size=16384,\n",
        "              epochs=5,\n",
        "              verbose=1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}